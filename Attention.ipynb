{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "USE_CUDA = torch.cuda.is_available()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_vocab(src):\n",
    "    word2idx = {}\n",
    "    idx2word = {}\n",
    "    for i,w in enumerate(open(src).read().splitlines()):\n",
    "        if w not in word2idx:\n",
    "            word2idx[w] = i\n",
    "            idx2word[i] = w\n",
    "    return word2idx, idx2word\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_vocab_src = \"./Data/vocab.en.txt\"\n",
    "vi_vocab_src = \"./Data/vocab.vi.txt\"\n",
    "train_en_src = \"./Data/train.en.txt\"\n",
    "train_vi_src = \"./Data/train.vi.txt\"\n",
    "valid_en_src = \"./Data/valid.en.txt\"\n",
    "valid_vi_src = \"./Data/valid.vi.txt\"\n",
    "test_en_src = \"./Data/tst2012.en.txt\"\n",
    "test_vi_src = \"./Data/tst2012.vi.txt\"\n",
    "\n",
    "source_vocab, idx2source = read_vocab(en_vocab_src)\n",
    "target_vocab, idx2target = read_vocab(vi_vocab_src)\n",
    " \n",
    "MAX_LEN = 100 # \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pdb\n",
    "# s_data = open(valid_en_src, \"rb\")\n",
    "# for l in s_data:\n",
    "#     print(l.split())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_iterator(s_src, t_src, s_vocab, t_vocab, max_sent_len=MAX_LEN, batch_size=1, num_sample=0):\n",
    "    s_data = open(s_src, \"r\").readlines()\n",
    "    t_data = open(t_src, \"r\").readlines()\n",
    "    if num_sample:\n",
    "        idx = random.sample(range(len(s_data)), num_sample)\n",
    "        s_data = np.array(s_data)[idx]\n",
    "        t_data = np.array(t_data)[idx]\n",
    "    f = lambda x: Variable(torch.LongTensor(x).view(1,-1))\n",
    "    out_source, out_target, len_source, len_target = [], [], [], []\n",
    "    batch_idx = 0\n",
    "    for i, (s_line, t_line) in enumerate(zip(s_data, t_data)):\n",
    "        if i - batch_idx >= batch_size:\n",
    "            yield out_source, out_target, len_source, len_target\n",
    "            out_source, out_target, len_source, len_target = [], [], [], []\n",
    "            batch_idx = i\n",
    "        a_source = [ s_vocab[w] if w in s_vocab else s_vocab[\"<unk>\"] \n",
    "                      for w in s_line.replace(\"\\n\", \"\").split(\" \")][:max_sent_len] ## could do reverse the input\n",
    "        a_target = [ t_vocab[w] if w in t_vocab else t_vocab[\"<unk>\"] \n",
    "                      for w in t_line.replace(\"/n\", \"</s>\").split()]\n",
    "        a_target.insert(0,t_vocab[\"<s>\"])\n",
    "        var_source = f(a_source).cuda() if USE_CUDA else f(a_source)\n",
    "        var_target = f(a_target).cuda() if USE_CUDA else f(a_target)\n",
    "        out_source.append(var_source)\n",
    "        out_target.append(var_target)\n",
    "        if (i+1)%batch_size == 0:\n",
    "            yield (out_source), (out_target), len_source, len_target\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# data = data_iterator(train_en_src, train_vi_src,source_vocab, target_vocab)\n",
    "# for d, s,_,_ in data:\n",
    "#     print(\"source len:\", len(d))\n",
    "#     print(\"target len:\", len(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers=1, bidirectional=False):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.bi = bidirectional\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, num_layers = num_layers, bidirectional = self.bi)\n",
    "\n",
    "    def forward(self, word_inputs, hidden=None):\n",
    "        seq_len = len(word_inputs)\n",
    "        embedded = self.embedding(word_inputs).view(seq_len, 1, -1)\n",
    "        output, hidden = self.gru(embedded, hidden)\n",
    "        return output, hidden\n",
    "    \n",
    "    def initHidden(self):\n",
    "        direction = 2 if self.bi else 1\n",
    "        result = Variable(torch.zeros(self.num_layers*direction, 1, self.hidden_size))\n",
    "        if USE_CUDA:\n",
    "            return result.cuda()\n",
    "        else:\n",
    "            return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(Attention, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.atten = nn.Linear(2*hidden_size, hidden_size)\n",
    "        self.w = nn.Parameter(torch.FloatTensor(1, hidden_size))\n",
    "        self.softmax = nn.Softmax(dim=0)\n",
    "        \n",
    "    \n",
    "    def forward(self, encoder_outputs, hidden):\n",
    "        source_len = len(encoder_outputs)\n",
    "        atten_all = Variable(torch.zeros(source_len))\n",
    "        if USE_CUDA:\n",
    "            atten_all = atten_all.cuda()\n",
    "\n",
    "        for i in range(source_len):\n",
    "            atten_all[i] = self.score(hidden.unsqueeze(0), encoder_outputs[i])\n",
    "        return self.softmax(atten_all).unsqueeze(0).unsqueeze(0)\n",
    "    \n",
    "    def score(self, hidden, encoder_outputs):\n",
    "        res = self.atten(torch.cat((hidden.view(1,-1), encoder_outputs), 1)) ## linear\n",
    "        res = self.w.dot(res)\n",
    "        return res\n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, num_layers=1):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size*2, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size*2, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "        self.atten = Attention(hidden_size)\n",
    "\n",
    "    def forward(self, input, last_context, last_hidden, encoder_outputs):\n",
    "        embedded = self.embedding(input).view(1,1,-1)\n",
    "       \n",
    "        rnn_input = torch.cat((embedded, last_context.unsqueeze(0)), 2)  # combine embedding and last context\n",
    "        rnn_output, hidden = self.gru(rnn_input, last_hidden)\n",
    "        weights = self.atten(encoder_outputs, hidden) # get weights\n",
    "        context = weights.bmm(encoder_outputs.transpose(0, 1)) \n",
    "        \n",
    "        rnn_output = rnn_output.squeeze(0) # 1 x B x N -> B x N\n",
    "        context = context.squeeze(1)       # B x 1 x N -> B x N\n",
    "        \n",
    "        output = self.softmax(self.out(torch.cat((rnn_output, context), 1)))\n",
    "        return output, context, hidden, weights\n",
    "\n",
    "    def initHidden(self):\n",
    "        result = Variable(torch.zeros(self.num_layers, 1, self.hidden_size))\n",
    "        if USE_CUDA:\n",
    "            return result.cuda()\n",
    "        else:\n",
    "            return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one(encoder, decoder, criterion, encoder_opt, decoder_opt, atten_opt, debug=True):\n",
    "    total_len = 0\n",
    "    total_loss = 0\n",
    "    data = None\n",
    "    if debug:\n",
    "        data = data_iterator(valid_en_src, valid_vi_src,source_vocab, target_vocab)\n",
    "    else:\n",
    "        data =  data_iterator(train_en_src, train_vi_src,source_vocab, target_vocab)\n",
    "\n",
    "    for source, target, _, _ in data:\n",
    "        for s, t in zip(source, target):\n",
    "            s = s.view(-1)\n",
    "            t = t.view(-1)\n",
    "            target_len = t.size()[0]\n",
    "            \n",
    "            encoder_hidden = encoder.initHidden()\n",
    "            encoder_opt.zero_grad()\n",
    "            decoder_opt.zero_grad()\n",
    "        \n",
    "            encoder_outputs = Variable(torch.zeros(MAX_LEN, encoder.hidden_size))\n",
    "            encoder_outputs = encoder_outputs.cuda() if USE_CUDA else encoder_outputs\n",
    "            encoder_output = None\n",
    "            \n",
    "            loss = 0\n",
    "            criterion = nn.NLLLoss()\n",
    "            encoder_outputs, encoder_hidden = encoder(s, encoder_hidden)\n",
    "            decoder_hidden = encoder_hidden\n",
    "            decoder_context = Variable(torch.zeros(1, decoder.hidden_size))\n",
    "            decoder_context = decoder_context.cuda() if USE_CUDA else decoder_context\n",
    "            \n",
    "            for de_i in range(target_len):\n",
    "                decoder_output, context, hidden, weights = decoder(t[de_i], decoder_context, decoder_hidden, encoder_outputs)\n",
    "                loss += criterion(decoder_output, t[de_i]) # need 2D of output\n",
    "                decoder_context = context\n",
    "                decoder_hidden = hidden\n",
    "        \n",
    "            loss.backward() \n",
    "            encoder_opt.step()\n",
    "            decoder_opt.step()\n",
    "            atten_opt.step()\n",
    "            \n",
    "            total_len += target_len\n",
    "            total_loss += loss.data[0]\n",
    "            \n",
    "    if total_len == 0:\n",
    "        return 0\n",
    "    return total_loss/total_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, en_sentence, vi_sentence):\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "    encoder_outputs, encoder_hidden = encoder(en_sentence, encoder_hidden)\n",
    "    \n",
    "    # Create starting vectors for decoder\n",
    "    decoder_input = Variable(torch.LongTensor([target_vocab[\"<s>\"]])) # start\n",
    "    decoder_context = Variable(torch.zeros(1, decoder.hidden_size))\n",
    "    if USE_CUDA:\n",
    "        decoder_input = decoder_input.cuda()\n",
    "        decoder_context = decoder_context.cuda()\n",
    "\n",
    "    decoder_hidden = encoder_hidden \n",
    "    decoded_words = []\n",
    "    criterion = nn.NLLLoss()\n",
    "    loss  = 0\n",
    "    # Run through decoder\n",
    "    for di in range(MAX_LEN):\n",
    "        decoder_output, decoder_context, decoder_hidden, weights = decoder(decoder_input, decoder_context, decoder_hidden, encoder_outputs)\n",
    "        \n",
    "        # Choose top word from output\n",
    "        topv, topi = decoder_output.data.topk(1)\n",
    "        ni = topi[0][0]\n",
    "        loss += criterion(ni, vi_sentence[di])\n",
    "        if ni == target_vocab['</s>']:\n",
    "            break\n",
    "        else:\n",
    "            decoded_words.append(idx2target[ni])  \n",
    "        # Choose next input word\n",
    "        decoder_input = Variable(torch.LongTensor([[ni]]))\n",
    "        decoder_input = decoder_input.cuda()  if USE_CUDA else decoder_input\n",
    "        \n",
    "    return decoded_words, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(encoder, decoder, n_epoches=200, learning_rate=0.01,):\n",
    "    encoder_opt = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_opt = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "    atten_opt = optim.SGD(decoder.atten.parameters(), lr=learning_rate)\n",
    "       \n",
    "    criterion = nn.NLLLoss()\n",
    "    total_loss = 0\n",
    "    print_loss = 0\n",
    "    loss = 0\n",
    "    print_every_ep = 1\n",
    "    for ep in range(1, n_epoches+1):\n",
    "        loss = train_one(encoder, decoder, criterion, encoder_opt, decoder_opt, atten_opt)\n",
    "        total_loss += loss\n",
    "        print_loss += loss\n",
    "        \n",
    "        if ep % print_every_ep == 0:\n",
    "            loss_avg = print_loss/print_every_ep\n",
    "            print_loss = 0\n",
    "\n",
    "            data = data_iterator(train_en_src, train_vi_src,source_vocab, target_vocab, num_sample=1)\n",
    "            pdb.set_trace()\n",
    "            for test_en_batch, test_vi_batch, _, _ in data:\n",
    "                for test_en, test_vi in zip(test_en_batch, test_vi_batch):\n",
    "                    test_en = test_en.view(1,1,-1)\n",
    "                    test_vi = test_vi.view(1,1,-1)\n",
    "                    decodewords, test_loss = evaluate(encoder, decoder, test_en, test_vi)\n",
    "                    print(\"epoch:{}, train_loss:{}, test_loss:{}\".format(ep, loss_avg, test_loss))\n",
    "            \n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> <ipython-input-43-d2be1fc18275>(22)train()\n",
      "-> for test_en_batch, test_vi_batch, _, _ in data:\n",
      "(Pdb) c\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-8753400d5918>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# encoder = EncoderRNN(1000, hidden_size)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# decoder = DecoderRNN(hidden_size,1000)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-43-d2be1fc18275>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(encoder, decoder, n_epoches, learning_rate)\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_en_src\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_vi_src\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msource_vocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_vocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_sample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mtest_en_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_vi_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mtest_en\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_vi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_en_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_vi_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m                     \u001b[0mtest_en\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_en\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-42-a762a92f7cc9>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(encoder, decoder, en_sentence, vi_sentence)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0men_sentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvi_sentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mencoder_hidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitHidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_hidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0men_sentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_hidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# Create starting vectors for decoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/WeiFu/miniconda2/envs/python3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-27-acb1f7585d8a>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, word_inputs, hidden)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mseq_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0membedded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgru\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/WeiFu/miniconda2/envs/python3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/WeiFu/miniconda2/envs/python3/lib/python3.6/site-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m             \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm_type\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m         )\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/WeiFu/miniconda2/envs/python3/lib/python3.6/site-packages/torch/nn/_functions/thnn/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(cls, ctx, indices, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mneeds_input_grad\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Embedding doesn't \"\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0;34m\"compute the gradient w.r.t. the indices\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "### Training\n",
    "\n",
    "hidden_size = 256\n",
    "encoder = EncoderRNN(len(source_vocab), hidden_size)\n",
    "decoder = DecoderRNN(hidden_size, len(target_vocab))\n",
    "# encoder = EncoderRNN(1000, hidden_size)\n",
    "# decoder = DecoderRNN(hidden_size,1000)\n",
    "train(encoder, decoder)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:python3]",
   "language": "python",
   "name": "conda-env-python3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
